# DigitalGrub Content Management Portal - Project Specification

**Version:** 1.0  
**Date:** November 6, 2025  
**Status:** Draft for Review

---

## ğŸ“‹ Executive Summary

A React-based web portal for managing crawlers, monitoring indexing status, and generating AI-powered blog content using indexed job portal data. The portal enables content developers to create blog titles and articles automatically from structured data (jobs, news, AI jobs) using LlamaIndex + Azure OpenAI.

### Key Objectives
1. **Crawler Management**: Monitor and control crawler status/schedules
2. **Indexing Dashboard**: View indexing progress and data quality metrics
3. **Content Generation**: AI-powered blog creation with title suggestions
4. **Data Exploration**: Search and analyze indexed content

---

## ğŸ—ï¸ Architecture Overview

### Technology Stack

#### Frontend
- **Framework**: React 18.3+ with TypeScript 5.6+
- **State Management**: TanStack Query v5 (React Query) + Zustand v5
- **UI Library**: Material-UI (MUI) v6 or Ant Design v5.21+
- **Routing**: React Router v6.28+
- **Data Fetching**: Axios 1.7+ with TanStack Query v5
- **Charts/Visualizations**: Recharts v2.13+ or Chart.js v4.4+
- **Code Editor**: Monaco Editor v0.52+ (for blog editing)
- **Build Tool**: Vite 6.0+
- **Styling**: Tailwind CSS v3.4+ + CSS Modules

#### Backend API
- **Framework**: FastAPI 0.115+ (Python 3.11+)
- **ASGI Server**: Uvicorn 0.32+ with uvloop
- **API Type**: RESTful JSON API
- **Authentication**: JWT tokens with refresh mechanism (python-jose 3.3+)
- **CORS**: Configured for React dev server + production domain
- **Validation**: Pydantic v2.9+
- **Documentation**: Auto-generated OpenAPI/Swagger UI

#### Database
- **Primary DB**: PostgreSQL 16+ with PgVector 0.8+ extension
- **ORM**: SQLAlchemy 2.0.35+ (existing models)
- **Connection Pool**: psycopg2-binary 2.9.10+ or asyncpg 0.30+ (async)
- **Migrations**: Alembic 1.14+ (existing setup)
- **Vector Storage**: PgVector 0.8+ (existing llamaindex_embedding_* tables)

#### AI/ML Services
- **Indexing**: LlamaIndex 0.12+ (existing)
- **Vector Store**: llama-index-vector-stores-postgres 0.3+
- **Azure Integration**: llama-index-embeddings-azure-openai 0.3+, llama-index-llms-azure-openai 0.3+
- **Embeddings**: Azure OpenAI text-embedding-3-large (3072-dim) (existing)
- **Content Generation**: Azure OpenAI GPT-4o/GPT-4-turbo (existing model-router)
- **Query Engine**: RAG with PgVector 0.8+ similarity search (existing)

#### Deployment
- **Frontend**: Vercel / Netlify / Azure Static Web Apps
- **Backend**: Azure App Service / Docker container
- **Database**: Existing PostgreSQL (20.244.82.149:5432)
- **Reverse Proxy**: Nginx (optional, for production)

---

## ğŸ“ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    React Frontend (Port 5173)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Dashboard  â”‚  Crawlers   â”‚   Indexing   â”‚  Content    â”‚ â”‚
â”‚  â”‚   Home      â”‚  Management â”‚   Monitor    â”‚  Generator  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ REST API (JSON)
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               FastAPI Backend (Port 8000)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Crawler API  â”‚ Indexing API â”‚  Content Generation API  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Existing Python Modules                                â”‚ â”‚
â”‚  â”‚  â€¢ indexer.py (JobIndexer, TNNewsIndexer, AIJobIndexer) â”‚ â”‚
â”‚  â”‚  â€¢ query_engine.py (ContentGenerator)                   â”‚ â”‚
â”‚  â”‚  â€¢ models.py (SQLAlchemy models)                        â”‚ â”‚
â”‚  â”‚  â€¢ config.py (Settings management)                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ SQLAlchemy
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          PostgreSQL + PgVector (20.244.82.149)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  jobs      â”‚  tnnews    â”‚  aijobs                    â”‚   â”‚
â”‚  â”‚  (33 cols) â”‚            â”‚                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Vector Tables (3072 dimensions)                     â”‚   â”‚
â”‚  â”‚  â€¢ llamaindex_embedding_jobs                         â”‚   â”‚
â”‚  â”‚  â€¢ llamaindex_embedding_tnnews                       â”‚   â”‚
â”‚  â”‚  â€¢ llamaindex_embedding_aijobs                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Azure OpenAI (Embeddings + LLM)                â”‚
â”‚  â€¢ text-embedding-3-large (3072-dim)                        â”‚
â”‚  â€¢ model-router deployment                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Feature Specifications

### 1. Dashboard (Home Page)

**Purpose**: Overview of system health and key metrics

**Features**:
- **System Stats Cards**:
  - Total indexed records (jobs, tnnews, aijobs)
  - Unindexed records count
  - Last indexing run timestamp
  - Active crawlers count
  
- **Recent Activity Timeline**:
  - Recent indexing operations
  - Blog posts created
  - Crawler runs
  
- **Quick Actions**:
  - "Index New Records" button
  - "Generate Blog Title" button
  - "View All Crawlers" link

- **Charts**:
  - Indexing trends (last 7/30 days)
  - Content sources distribution (pie chart)
  - Blog generation activity

**API Endpoints**:
```
GET /api/v1/dashboard/stats
GET /api/v1/dashboard/recent-activity
GET /api/v1/dashboard/trends
```

**Mockup Flow**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DigitalGrub Portal        [ğŸ‘¤ Admin â–¼]     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“Š Overview Statistics                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  12,547 â”‚ â”‚   234   â”‚ â”‚    3    â”‚        â”‚
â”‚  â”‚ Indexed â”‚ â”‚Unindexedâ”‚ â”‚Crawlers â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                              â”‚
â”‚  ğŸ“ˆ Indexing Trend (Last 7 Days)             â”‚
â”‚  [Line Chart]                                â”‚
â”‚                                              â”‚
â”‚  ğŸ•’ Recent Activity                          â”‚
â”‚  â€¢ 2 min ago: Indexed 50 jobs               â”‚
â”‚  â€¢ 15 min ago: Generated blog "Top Tech..." â”‚
â”‚  â€¢ 1 hour ago: Crawler "jobs" completed     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2. Crawler Management

**Purpose**: Monitor crawler status, view logs, and manage crawler configurations

**Features**:
- **Crawler List Table**:
  - Columns: Name, Status, Last Run, Next Run, Records Added, Actions
  - Status badges (Running, Idle, Failed, Scheduled)
  - Sortable and filterable
  
- **Crawler Details Page**:
  - Configuration display (source URL, schedule, target table)
  - Execution history with logs
  - Manual trigger button
  - Enable/disable toggle
  
- **Crawler Logs Viewer**:
  - Real-time log streaming (WebSocket)
  - Filter by level (INFO, ERROR, WARNING)
  - Download logs as file

**Database Schema** (new tables):
```sql
CREATE TABLE crawlers (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) UNIQUE NOT NULL,
    description TEXT,
    target_table VARCHAR(50) NOT NULL,  -- 'jobs', 'tnnews', 'aijobs'
    source_url TEXT,
    schedule_cron VARCHAR(50),
    is_enabled BOOLEAN DEFAULT true,
    last_run_at TIMESTAMP,
    last_status VARCHAR(20),  -- 'success', 'failed', 'running'
    records_added INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE crawler_logs (
    id SERIAL PRIMARY KEY,
    crawler_id INTEGER REFERENCES crawlers(id),
    run_started_at TIMESTAMP NOT NULL,
    run_completed_at TIMESTAMP,
    status VARCHAR(20),
    records_crawled INTEGER DEFAULT 0,
    errors_count INTEGER DEFAULT 0,
    log_file_path TEXT,
    error_message TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);
```

**API Endpoints**:
```
GET    /api/v1/crawlers                 # List all crawlers
GET    /api/v1/crawlers/{id}            # Get crawler details
POST   /api/v1/crawlers                 # Create new crawler
PUT    /api/v1/crawlers/{id}            # Update crawler
DELETE /api/v1/crawlers/{id}            # Delete crawler
POST   /api/v1/crawlers/{id}/trigger    # Manual trigger
GET    /api/v1/crawlers/{id}/logs       # Get crawler logs
WS     /ws/crawlers/{id}/logs           # Real-time log stream
```

**Mockup Flow**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Crawlers Management         [+ Add Crawler] â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Name       â”‚Status â”‚Last Run    â”‚Recordsâ”‚âš™ï¸  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤
â”‚job-crawler â”‚ğŸŸ¢ Idleâ”‚2 hours ago â”‚  150  â”‚â–¶ï¸âš™ï¸â”‚
â”‚news-crawlerâ”‚ğŸ”µ Run â”‚Just now    â”‚   45  â”‚â¸ï¸âš™ï¸â”‚
â”‚ai-crawler  â”‚ğŸ”´ Failâ”‚1 day ago   â”‚    0  â”‚â–¶ï¸âš™ï¸â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[Click on crawler name opens details page]
```

---

### 3. Indexing Monitor

**Purpose**: Track indexing progress and data quality

**Features**:
- **Indexing Status Dashboard**:
  - Progress bars for each data source (jobs, tnnews, aijobs)
  - Indexing queue size
  - Average indexing time per record
  - Error rate
  
- **Manual Indexing Controls**:
  - "Index All Sources" button
  - Source-specific indexing (jobs only, news only, etc.)
  - Batch size configuration
  - Re-index specific records (by ID range)
  
- **Indexing History Table**:
  - Timestamp, Source, Records Processed, Duration, Status
  - View details (which records indexed, any errors)
  
- **Data Quality Metrics**:
  - Records with missing embeddings
  - Records with incomplete metadata
  - Vector dimension validation
  - Duplicate detection

**Database Schema** (new table):
```sql
CREATE TABLE indexing_runs (
    id SERIAL PRIMARY KEY,
    source VARCHAR(50) NOT NULL,  -- 'jobs', 'tnnews', 'aijobs'
    started_at TIMESTAMP NOT NULL,
    completed_at TIMESTAMP,
    status VARCHAR(20),  -- 'running', 'completed', 'failed'
    records_processed INTEGER DEFAULT 0,
    records_failed INTEGER DEFAULT 0,
    batch_size INTEGER DEFAULT 100,
    duration_seconds INTEGER,
    error_message TEXT,
    triggered_by VARCHAR(100),  -- 'manual', 'scheduled', 'auto'
    created_at TIMESTAMP DEFAULT NOW()
);
```

**API Endpoints**:
```
GET    /api/v1/indexing/status              # Current indexing status
POST   /api/v1/indexing/trigger             # Start indexing
GET    /api/v1/indexing/history             # Indexing history
GET    /api/v1/indexing/queue               # Current queue
GET    /api/v1/indexing/quality-check       # Data quality metrics
POST   /api/v1/indexing/reindex             # Reindex specific records
```

**Mockup Flow**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Indexing Monitor           [ğŸ”„ Index Now]   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“Š Current Status                           â”‚
â”‚  Jobs:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 80% (10,200/12,750)    â”‚
â”‚  News:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% (5,400/5,400)     â”‚
â”‚  AI Jobs: â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 20% (120/600)          â”‚
â”‚                                              â”‚
â”‚  âš¡ Quick Actions                            â”‚
â”‚  [Index Jobs Only] [Index All] [Re-index]   â”‚
â”‚                                              â”‚
â”‚  ğŸ“œ Recent Indexing Runs                     â”‚
â”‚  â€¢ 2h ago: jobs (150 records, 2.3min) âœ…    â”‚
â”‚  â€¢ 5h ago: tnnews (45 records, 1.1min) âœ…   â”‚
â”‚  â€¢ 1d ago: aijobs (0 records) âŒ Error      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 4. Content Generator

**Purpose**: AI-powered blog creation from indexed data

#### 4.1 Blog Title Generator

**Features**:
- **Input Form**:
  - Topic/keyword input
  - Data source selection (jobs, tnnews, aijobs, or "All Sources")
  - Number of suggestions slider (3-10)
  - Style selector (SEO-focused, Creative, Professional, Catchy)
  
- **Results Display**:
  - List of generated titles with scores
  - "Copy" button for each title
  - "Use This Title" â†’ Opens blog content generator
  - "Regenerate" button
  - "Save Favorite" option
  
- **History/Favorites**:
  - View previously generated titles
  - Save favorite titles for later

**API Endpoints**:
```
POST   /api/v1/content/generate-titles      # Generate blog titles
GET    /api/v1/content/title-history        # Title generation history
POST   /api/v1/content/save-favorite-title  # Save favorite title
```

**Mockup Flow**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generate Blog Title                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Topic: [Remote Tech Jobs in 2025________]   â”‚
â”‚                                              â”‚
â”‚  Data Source: [Jobs â–¼]                      â”‚
â”‚  Number of Suggestions: â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â—‹ 5        â”‚
â”‚  Style: [âš« SEO-focused  â—‹ Creative  â—‹...]   â”‚
â”‚                                              â”‚
â”‚  [ğŸ¯ Generate Titles]                        â”‚
â”‚                                              â”‚
â”‚  ğŸ“ Generated Titles:                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 1. "Top 10 Remote Tech Jobs Hiring    â”‚   â”‚
â”‚  â”‚     in 2025" [Copy] [Use This]        â”‚   â”‚
â”‚  â”‚ 2. "Remote Work Revolution: Best..."  â”‚   â”‚
â”‚  â”‚ 3. "High-Paying Remote Tech Roles..." â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.2 Blog Content Generator

**Features**:
- **Input Form**:
  - Title input (can use generated title)
  - Data source selection
  - Word count target (500-3000)
  - Writing style (Informative, Listicle, Analytical, How-to)
  - Tone selector (Professional, Casual, Technical)
  - Optional metadata filters (location, company, sector, etc.)
  
- **Content Editor**:
  - Monaco Editor with Markdown support
  - Live preview panel (split view)
  - AI suggestions panel (show retrieved data snippets)
  - Insert data button (add specific job examples)
  
- **Generated Content Display**:
  - Full blog content with formatting
  - Automatically generated tags
  - Summary/excerpt
  - Word count
  - SEO score (basic analysis)
  
- **Actions**:
  - Copy to clipboard
  - Export as Markdown/HTML
  - Save draft
  - Publish (if CMS integration exists)
  - Regenerate sections

**Database Schema** (new table):
```sql
CREATE TABLE blog_posts (
    id SERIAL PRIMARY KEY,
    title VARCHAR(500) NOT NULL,
    content TEXT NOT NULL,
    content_html TEXT,
    summary TEXT,
    tags TEXT[],
    data_source VARCHAR(50),  -- 'jobs', 'tnnews', 'aijobs', 'mixed'
    word_count INTEGER,
    style VARCHAR(50),
    status VARCHAR(20) DEFAULT 'draft',  -- 'draft', 'published'
    created_by VARCHAR(100),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    published_at TIMESTAMP
);

CREATE TABLE generation_history (
    id SERIAL PRIMARY KEY,
    blog_post_id INTEGER REFERENCES blog_posts(id),
    generation_type VARCHAR(50),  -- 'title', 'content', 'section'
    input_params JSONB,
    model_used VARCHAR(100),
    tokens_used INTEGER,
    generation_time_seconds FLOAT,
    created_at TIMESTAMP DEFAULT NOW()
);
```

**API Endpoints**:
```
POST   /api/v1/content/generate-blog        # Generate full blog
POST   /api/v1/content/generate-section     # Regenerate specific section
GET    /api/v1/content/similar-content      # Search similar indexed content
POST   /api/v1/content/save-draft           # Save blog draft
GET    /api/v1/content/drafts               # List saved drafts
PUT    /api/v1/content/drafts/{id}          # Update draft
DELETE /api/v1/content/drafts/{id}          # Delete draft
POST   /api/v1/content/publish              # Publish blog
```

**Mockup Flow**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generate Blog Content                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Title: [Top 10 Remote Tech Jobs 2025____]   â”‚
â”‚  Source: [Jobs â–¼]  Words: [â—â”€â”€â”€â”€] 1000      â”‚
â”‚  Style: [Listicle â–¼]  Tone: [Professional]  â”‚
â”‚                                              â”‚
â”‚  Filters (Optional):                         â”‚
â”‚  Location: [Any â–¼]  Sector: [Tech â–¼]        â”‚
â”‚                                              â”‚
â”‚  [ğŸ¨ Generate Blog]                          â”‚
â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Editor          â”‚ Preview            â”‚   â”‚
â”‚  â”‚                 â”‚                    â”‚   â”‚
â”‚  â”‚ # Top 10 Remote â”‚ [Rendered HTML]    â”‚   â”‚
â”‚  â”‚                 â”‚                    â”‚   â”‚
â”‚  â”‚ [AI-generated   â”‚                    â”‚   â”‚
â”‚  â”‚  content...]    â”‚                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                              â”‚
â”‚  ğŸ“Š Generated: 1,024 words | 8 tags          â”‚
â”‚  [ğŸ’¾ Save Draft] [ğŸ“‹ Copy] [ğŸ“¤ Export]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.3 Additional Content Tools

**Trend Analysis**:
- Analyze trends for specific topics
- Time-based filtering
- Regional variations
- Salary trends
- Skills demand analysis

**Content Comparison**:
- Compare two topics (e.g., "Data Scientist vs Data Engineer")
- Side-by-side comparison generator

**Smart Search**:
- Semantic search across indexed data
- Find similar jobs/news/content
- Filter by metadata

**API Endpoints**:
```
POST   /api/v1/content/trend-analysis       # Generate trend analysis
POST   /api/v1/content/compare              # Generate comparison
POST   /api/v1/content/search               # Semantic search
```

---

### 5. User Management (Phase 2)

**Features**:
- User authentication (login/logout)
- Role-based access control (Admin, Content Creator, Viewer)
- User profile management
- Activity audit logs

**Database Schema**:
```sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    full_name VARCHAR(100),
    role VARCHAR(20) DEFAULT 'content_creator',  -- 'admin', 'content_creator', 'viewer'
    is_active BOOLEAN DEFAULT true,
    last_login_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE audit_logs (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id),
    action VARCHAR(100),
    resource_type VARCHAR(50),
    resource_id INTEGER,
    details JSONB,
    ip_address VARCHAR(45),
    created_at TIMESTAMP DEFAULT NOW()
);
```

**API Endpoints**:
```
POST   /api/v1/auth/login                   # User login
POST   /api/v1/auth/logout                  # User logout
POST   /api/v1/auth/refresh                 # Refresh JWT token
GET    /api/v1/users/me                     # Current user profile
GET    /api/v1/users                        # List users (admin only)
POST   /api/v1/users                        # Create user (admin only)
PUT    /api/v1/users/{id}                   # Update user
```

---

## ğŸ—‚ï¸ Project Structure

### Monorepo Structure

```
digitalgrub-indexer/
â”œâ”€â”€ backend/                          # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                  # FastAPI app entry
â”‚   â”‚   â”œâ”€â”€ config.py                # Settings (existing)
â”‚   â”‚   â”œâ”€â”€ models.py                # DB models (existing)
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ deps.py              # Dependencies (DB session, auth)
â”‚   â”‚   â”‚   â””â”€â”€ v1/
â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚       â”œâ”€â”€ dashboard.py     # Dashboard endpoints
â”‚   â”‚   â”‚       â”œâ”€â”€ crawlers.py      # Crawler management
â”‚   â”‚   â”‚       â”œâ”€â”€ indexing.py      # Indexing endpoints
â”‚   â”‚   â”‚       â”œâ”€â”€ content.py       # Content generation
â”‚   â”‚   â”‚       â””â”€â”€ auth.py          # Authentication (Phase 2)
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ crawler_service.py   # Crawler logic
â”‚   â”‚   â”‚   â”œâ”€â”€ indexing_service.py  # Indexing logic (wraps existing)
â”‚   â”‚   â”‚   â””â”€â”€ content_service.py   # Content generation (wraps existing)
â”‚   â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ crawler.py           # Pydantic schemas
â”‚   â”‚   â”‚   â”œâ”€â”€ indexing.py
â”‚   â”‚   â”‚   â””â”€â”€ content.py
â”‚   â”‚   â””â”€â”€ websockets/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ crawler_logs.py      # WebSocket for logs
â”‚   â”œâ”€â”€ indexer.py                   # Existing indexing code
â”‚   â”œâ”€â”€ query_engine.py              # Existing query engine
â”‚   â”œâ”€â”€ migrations/                  # Alembic migrations (existing)
â”‚   â”œâ”€â”€ requirements.txt             # Python dependencies
â”‚   â””â”€â”€ .env                         # Environment variables (existing)
â”‚
â”œâ”€â”€ frontend/                         # React frontend
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â”œâ”€â”€ favicon.ico
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.tsx                 # Entry point
â”‚   â”‚   â”œâ”€â”€ App.tsx                  # Root component
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ client.ts            # Axios instance
â”‚   â”‚   â”‚   â”œâ”€â”€ dashboard.ts         # Dashboard API calls
â”‚   â”‚   â”‚   â”œâ”€â”€ crawlers.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ indexing.ts
â”‚   â”‚   â”‚   â””â”€â”€ content.ts
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ layout/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ AppLayout.tsx    # Main layout with sidebar
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Sidebar.tsx
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Header.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ dashboard/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ StatsCard.tsx
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ActivityTimeline.tsx
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ TrendChart.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ crawlers/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ CrawlerList.tsx
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ CrawlerDetails.tsx
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ CrawlerLogs.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ indexing/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ IndexingStatus.tsx
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ IndexingHistory.tsx
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ QualityMetrics.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ content/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ TitleGenerator.tsx
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ BlogEditor.tsx
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ContentPreview.tsx
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ SavedDrafts.tsx
â”‚   â”‚   â”‚   â””â”€â”€ common/
â”‚   â”‚   â”‚       â”œâ”€â”€ LoadingSpinner.tsx
â”‚   â”‚   â”‚       â”œâ”€â”€ ErrorBoundary.tsx
â”‚   â”‚   â”‚       â””â”€â”€ ConfirmDialog.tsx
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Crawlers.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Indexing.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ContentGenerator.tsx
â”‚   â”‚   â”‚   â””â”€â”€ Login.tsx            # Phase 2
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â”œâ”€â”€ useDashboard.ts      # React Query hooks
â”‚   â”‚   â”‚   â”œâ”€â”€ useCrawlers.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ useIndexing.ts
â”‚   â”‚   â”‚   â””â”€â”€ useContent.ts
â”‚   â”‚   â”œâ”€â”€ store/
â”‚   â”‚   â”‚   â””â”€â”€ authStore.ts         # Zustand store (Phase 2)
â”‚   â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â”‚   â”œâ”€â”€ crawler.ts           # TypeScript types
â”‚   â”‚   â”‚   â”œâ”€â”€ indexing.ts
â”‚   â”‚   â”‚   â””â”€â”€ content.ts
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”œâ”€â”€ formatters.ts        # Date, number formatting
â”‚   â”‚   â”‚   â””â”€â”€ constants.ts
â”‚   â”‚   â””â”€â”€ styles/
â”‚   â”‚       â””â”€â”€ globals.css
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â””â”€â”€ .env.local                   # Frontend env vars
â”‚
â”œâ”€â”€ crawlers/                         # Crawler scripts (from developers)
â”‚   â”œâ”€â”€ job_crawler.py
â”‚   â”œâ”€â”€ news_crawler.py
â”‚   â”œâ”€â”€ ai_job_crawler.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ API.md                       # API documentation
â”‚   â”œâ”€â”€ DEPLOYMENT.md                # Deployment guide
â”‚   â””â”€â”€ DEVELOPMENT.md               # Development setup
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ seed_crawlers.py             # Seed initial crawler configs
â”‚   â””â”€â”€ backup_db.sh
â”‚
â”œâ”€â”€ README.md                        # Existing main README
â”œâ”€â”€ PROJECT_SPEC.md                  # This document
â””â”€â”€ docker-compose.yml               # For local development (optional)
```

---

## ğŸš€ Implementation Phases

### Phase 1: MVP (2-3 weeks)
**Goal**: Core functionality for content generation

**Backend**:
- âœ… FastAPI project setup with basic structure
- âœ… Wrap existing indexer.py and query_engine.py as services
- âœ… Implement Dashboard API (stats, activity)
- âœ… Implement Indexing API (trigger, status, history)
- âœ… Implement Content Generation API (titles, blog content)
- âœ… Add CORS middleware
- âœ… Create OpenAPI documentation

**Frontend**:
- âœ… React + TypeScript project with Vite
- âœ… Setup routing (React Router)
- âœ… Create layout with sidebar navigation
- âœ… Dashboard page with stats cards
- âœ… Indexing Monitor page with manual trigger
- âœ… Blog Title Generator page
- âœ… Blog Content Generator page with Monaco Editor
- âœ… API integration with React Query

**Database**:
- âœ… Create new tables: indexing_runs, blog_posts, generation_history
- âœ… Alembic migration for new tables

**Testing**:
- Basic manual testing
- Test content generation with sample data

**Deliverables**:
- Working portal accessible at http://localhost:5173
- Backend API at http://localhost:8000
- Ability to generate blog titles and content
- Ability to trigger indexing manually

---

### Phase 2: Crawler Management (1-2 weeks)
**Goal**: Integrate crawler management

**Backend**:
- âœ… Implement Crawler Management API
- âœ… WebSocket endpoint for real-time logs
- âœ… Crawler execution service
- âœ… Integration with crawler scripts from developers

**Frontend**:
- âœ… Crawler list page with status badges
- âœ… Crawler details page
- âœ… Real-time log viewer (WebSocket)
- âœ… Manual crawler trigger functionality

**Database**:
- âœ… Create tables: crawlers, crawler_logs
- âœ… Alembic migration

**Testing**:
- Test crawler triggers
- Test log streaming

**Deliverables**:
- Full crawler management interface
- Real-time monitoring of crawler execution

---

### Phase 3: Authentication & User Management (1 week)
**Goal**: Secure the portal with user authentication

**Backend**:
- âœ… JWT authentication implementation
- âœ… User management API
- âœ… Role-based access control middleware
- âœ… Audit logging

**Frontend**:
- âœ… Login page
- âœ… Authentication state management (Zustand)
- âœ… Protected routes
- âœ… User profile display

**Database**:
- âœ… Create tables: users, audit_logs
- âœ… Alembic migration

**Testing**:
- Test authentication flow
- Test authorization (different user roles)

**Deliverables**:
- Secure portal with login system
- Multi-user support

---

### Phase 4: Enhancements & Polish (1-2 weeks)
**Goal**: Improve UX and add advanced features

**Features**:
- Advanced content search/filter
- Trend analysis visualization
- Comparison tools
- Export functionality (PDF, Markdown, HTML)
- Batch blog generation
- Scheduled content generation
- Email notifications
- Dark mode theme
- Mobile responsive design

**Testing**:
- End-to-end testing
- Performance optimization
- Security audit

**Deliverables**:
- Production-ready portal
- Deployment documentation

---

## ğŸ”Œ API Specifications

### Base URL
```
Development: http://localhost:8000/api/v1
Production: https://api.digitalgrub.com/api/v1
```

### Authentication (Phase 3)
```
Authorization: Bearer <JWT_TOKEN>
```

### Common Response Format

**Success Response**:
```json
{
  "status": "success",
  "data": { ... },
  "message": "Operation completed successfully"
}
```

**Error Response**:
```json
{
  "status": "error",
  "error": {
    "code": "ERROR_CODE",
    "message": "Human-readable error message",
    "details": { ... }
  }
}
```

### Sample API Endpoints

#### Dashboard Stats
```
GET /api/v1/dashboard/stats

Response:
{
  "status": "success",
  "data": {
    "total_indexed": {
      "jobs": 12547,
      "tnnews": 5400,
      "aijobs": 600
    },
    "unindexed": {
      "jobs": 234,
      "tnnews": 0,
      "aijobs": 120
    },
    "last_indexing_run": "2025-11-06T10:30:00Z",
    "active_crawlers": 3
  }
}
```

#### Trigger Indexing
```
POST /api/v1/indexing/trigger

Request Body:
{
  "source": "jobs",  // or "tnnews", "aijobs", "all"
  "batch_size": 100,
  "force_reindex": false  // optional
}

Response:
{
  "status": "success",
  "data": {
    "run_id": 123,
    "status": "running",
    "started_at": "2025-11-06T11:00:00Z"
  }
}
```

#### Generate Blog Titles
```
POST /api/v1/content/generate-titles

Request Body:
{
  "topic": "Remote Tech Jobs in 2025",
  "source": "jobs",
  "num_suggestions": 5,
  "style": "seo_focused"
}

Response:
{
  "status": "success",
  "data": {
    "titles": [
      {
        "id": 1,
        "title": "Top 10 Remote Tech Jobs Hiring in 2025",
        "score": 0.95
      },
      {
        "id": 2,
        "title": "Remote Work Revolution: Best Tech Opportunities in 2025",
        "score": 0.92
      },
      ...
    ],
    "generation_time": 2.3
  }
}
```

#### Generate Blog Content
```
POST /api/v1/content/generate-blog

Request Body:
{
  "title": "Top 10 Remote Tech Jobs Hiring in 2025",
  "source": "jobs",
  "word_count": 1000,
  "style": "listicle",
  "tone": "professional",
  "filters": {
    "location": "Remote",
    "sector": "Technology"
  }
}

Response:
{
  "status": "success",
  "data": {
    "title": "Top 10 Remote Tech Jobs Hiring in 2025",
    "content": "# Top 10 Remote Tech Jobs...\n\n...",
    "content_html": "<h1>Top 10 Remote Tech Jobs...</h1>...",
    "summary": "Discover the most in-demand remote tech positions...",
    "tags": ["remote work", "tech jobs", "2025", "hiring"],
    "word_count": 1024,
    "generation_time": 8.5,
    "sources_used": 15
  }
}
```

---

## ğŸ¨ UI/UX Design Guidelines

### Design System
- **Color Palette**:
  - Primary: #1976D2 (Blue)
  - Secondary: #DC004E (Pink)
  - Success: #4CAF50 (Green)
  - Warning: #FF9800 (Orange)
  - Error: #F44336 (Red)
  - Background: #F5F5F5 (Light Gray)
  - Surface: #FFFFFF (White)

- **Typography**:
  - Headings: Inter, sans-serif
  - Body: Roboto, sans-serif
  - Code: Fira Code, monospace

- **Spacing**: 8px base unit (8, 16, 24, 32, 40, 48, 64)

### Component Guidelines
- Use Material-UI or Ant Design components
- Consistent button styles (primary, secondary, outlined)
- Loading states for all async operations
- Empty states with helpful messages
- Error states with retry options
- Success notifications (toast/snackbar)

### Responsive Design
- Desktop-first approach
- Breakpoints:
  - Desktop: 1200px+
  - Tablet: 768px - 1199px
  - Mobile: < 768px

---

## ğŸ”’ Security Considerations

### Authentication & Authorization
- JWT tokens with 1-hour expiry
- Refresh tokens with 7-day expiry
- Password hashing with bcrypt
- Role-based access control (RBAC)

### API Security
- Rate limiting (100 req/min per IP)
- Input validation with Pydantic
- SQL injection prevention (SQLAlchemy ORM)
- XSS prevention (sanitize user input)
- CORS configuration (whitelist frontend domain)

### Data Security
- Environment variables for secrets (.env)
- HTTPS only in production
- Database connection encryption (SSL)
- Audit logs for sensitive operations

### Azure OpenAI Security
- API key stored in environment variables
- Key rotation strategy
- Usage monitoring to prevent abuse

---

## ğŸ“Š Performance Requirements

### Backend
- API response time: < 500ms (95th percentile)
- Content generation: < 10s per blog post
- Indexing: 100 records/minute minimum
- Concurrent users: Support 50+ simultaneous users

### Frontend
- Initial page load: < 3s
- Time to Interactive (TTI): < 5s
- Lighthouse score: > 90
- Bundle size: < 500KB (gzipped)

### Database
- Query response time: < 100ms (simple queries)
- Vector search: < 2s (for 10K+ records)
- Connection pool: 10-20 connections

---

## ğŸ§ª Testing Strategy

### Backend Testing
- Unit tests (pytest)
  - Service layer tests
  - Model tests
  - Utility function tests
  
- Integration tests
  - API endpoint tests
  - Database integration tests
  - LlamaIndex integration tests
  
- Load tests (Locust)
  - API load testing
  - Concurrent indexing tests

### Frontend Testing
- Unit tests (Vitest)
  - Component tests
  - Hook tests
  - Utility function tests
  
- Integration tests (React Testing Library)
  - Page tests
  - User flow tests
  
- End-to-end tests (Playwright/Cypress)
  - Critical user journeys
  - Authentication flow
  - Content generation flow

### Manual Testing
- Cross-browser testing (Chrome, Firefox, Safari)
- Mobile responsiveness testing
- Accessibility testing (WCAG 2.1 AA)

---

## ğŸš€ Deployment Strategy

### Development Environment
- Frontend: Vite dev server (http://localhost:5173)
- Backend: Uvicorn dev server (http://localhost:8000)
- Database: Existing PostgreSQL (20.244.82.149)

### Staging Environment
- Frontend: Vercel preview deployment
- Backend: Azure App Service (staging slot)
- Database: Same as production (with separate schema)

### Production Environment
- **Frontend**: 
  - Vercel / Netlify / Azure Static Web Apps
  - CDN for static assets
  - Custom domain (e.g., portal.digitalgrub.com)
  
- **Backend**:
  - Azure App Service (Linux, Python 3.11)
  - or Docker container on Azure Container Instances
  - Auto-scaling enabled
  - Custom domain (e.g., api.digitalgrub.com)
  
- **Database**:
  - Existing PostgreSQL (20.244.82.149)
  - Regular backups
  - Connection pooling (PgBouncer)

### CI/CD Pipeline
- **GitHub Actions**:
  - Run tests on PR
  - Build frontend on merge to main
  - Deploy backend to Azure
  - Deploy frontend to Vercel
  
- **Deployment Steps**:
  1. Run linter and type checks
  2. Run unit tests
  3. Build production bundle
  4. Deploy to staging
  5. Run smoke tests
  6. Deploy to production
  7. Send notification to team

---

## ğŸ“ Environment Variables

### Backend (.env)
```env
# Database
DB_HOST=20.244.82.149
DB_PORT=5432
DB_NAME=booksgrub_index_sources
DB_USER=postgres
DB_PASSWORD=your_password

# Azure OpenAI
AZURE_OPENAI_API_KEY=your_key
AZURE_OPENAI_ENDPOINT=https://ennkantitham-resource.cognitiveservices.azure.com/
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-large
AZURE_OPENAI_LLM_DEPLOYMENT=model-router
AZURE_OPENAI_API_VERSION=2024-08-01-preview

# Vector Settings
VECTOR_DIMENSION=3072
VECTOR_TABLE_PREFIX=llamaindex_embedding

# API Settings
API_V1_PREFIX=/api/v1
DEBUG=false
LOG_LEVEL=INFO

# Security (Phase 3)
SECRET_KEY=your-secret-key-here
ACCESS_TOKEN_EXPIRE_MINUTES=60
REFRESH_TOKEN_EXPIRE_DAYS=7

# CORS
CORS_ORIGINS=http://localhost:5173,https://portal.digitalgrub.com
```

### Frontend (.env.local)
```env
VITE_API_BASE_URL=http://localhost:8000/api/v1
VITE_WS_BASE_URL=ws://localhost:8000/ws
```

---

## ğŸ› Error Handling

### Backend Error Codes
```
400 - Bad Request (invalid input)
401 - Unauthorized (missing/invalid token)
403 - Forbidden (insufficient permissions)
404 - Not Found (resource doesn't exist)
409 - Conflict (duplicate resource)
422 - Unprocessable Entity (validation error)
429 - Too Many Requests (rate limit exceeded)
500 - Internal Server Error
503 - Service Unavailable (maintenance mode)
```

### Frontend Error Handling
- Display user-friendly error messages
- Show toast notifications for errors
- Retry failed requests (with exponential backoff)
- Fallback UI for critical errors
- Error boundary for React component errors
- Log errors to console (dev) / monitoring service (prod)

---

## ğŸ“ˆ Monitoring & Observability

### Backend Monitoring
- Application logs (structured JSON logs)
- API metrics (request count, response time, error rate)
- Database query performance
- Azure OpenAI usage (token count, cost)
- System metrics (CPU, memory, disk)

### Frontend Monitoring
- Error tracking (Sentry / LogRocket)
- Performance monitoring (Web Vitals)
- User analytics (Google Analytics / Mixpanel)
- Session replay for debugging

### Alerts
- API error rate > 5%
- Response time > 2s (95th percentile)
- Database connection errors
- Disk space < 10%
- Crawler failures

---

## ğŸ”„ Future Enhancements

### Short-term (Next 3-6 months)
- Scheduled blog generation (cron jobs)
- Email notifications for indexing/crawler status
- Multi-language support (i18n)
- Advanced search with filters
- Bulk operations (batch blog generation)
- Content calendar view
- SEO score analysis

### Medium-term (6-12 months)
- Integration with CMS (WordPress, Ghost, etc.)
- Social media auto-posting
- A/B testing for blog titles
- Analytics dashboard (blog performance)
- Content recommendations
- Collaborative editing (multiple users)
- Version control for blog posts

### Long-term (12+ months)
- Custom AI model fine-tuning
- Voice-to-blog (speech input)
- Image generation for blog posts
- Multi-tenant support (SaaS model)
- Mobile app (React Native)
- Browser extension for quick blog creation

---

## âœ… Success Metrics

### Technical Metrics
- API uptime: > 99.5%
- Average indexing time: < 5 seconds per record
- Blog generation success rate: > 95%
- Frontend load time: < 3 seconds

### Business Metrics
- Blog posts created per week
- Time saved vs manual content creation
- User satisfaction score
- Crawler reliability (success rate)

### User Engagement
- Daily active users
- Average session duration
- Blog drafts saved
- Feature adoption rate

---

## ğŸ“ Support & Maintenance

### Documentation
- API documentation (auto-generated with FastAPI)
- User guide with screenshots
- Developer setup guide
- Troubleshooting guide

### Maintenance Windows
- Weekly: Database backups
- Monthly: Security updates
- Quarterly: Dependency updates

### Support Channels
- GitHub Issues (bug reports, feature requests)
- Internal Slack channel
- Email support

---

## ğŸ“ Learning Resources

### For Frontend Developers
- React Official Docs: https://react.dev
- TypeScript Handbook: https://www.typescriptlang.org/docs/
- React Query: https://tanstack.com/query/latest/docs/react/overview
- Material-UI: https://mui.com

### For Backend Developers
- FastAPI Docs: https://fastapi.tiangolo.com
- LlamaIndex Docs: https://docs.llamaindex.ai
- SQLAlchemy Docs: https://docs.sqlalchemy.org
- PgVector: https://github.com/pgvector/pgvector

---

## ğŸ“„ Appendix

### A. Database Schema Diagram
See separate `DATABASE_SCHEMA.md` document

### B. API Endpoint Reference
See separate `API_REFERENCE.md` document

### C. Wireframes & Mockups
See `wireframes/` directory

### D. Architecture Decision Records (ADR)
See `docs/adr/` directory

---

## ğŸ¤ Team & Responsibilities

### Roles
- **Backend Developer**: FastAPI API development, LlamaIndex integration
- **Frontend Developer**: React UI development, state management
- **DevOps Engineer**: Deployment, CI/CD, monitoring
- **Content Developer**: User testing, content requirements, QA

### Communication
- Daily standups (async in Slack)
- Weekly sprint planning
- Bi-weekly sprint retrospectives

---

## ğŸ“… Timeline Summary

| Phase | Duration | Key Deliverables |
|-------|----------|------------------|
| Phase 1 (MVP) | 2-3 weeks | Dashboard, Indexing, Content Generation |
| Phase 2 (Crawlers) | 1-2 weeks | Crawler Management, Real-time Logs |
| Phase 3 (Auth) | 1 week | User Authentication, RBAC |
| Phase 4 (Polish) | 1-2 weeks | Enhancements, Testing, Deployment |
| **Total** | **5-8 weeks** | Production-ready portal |

---

## âœï¸ Document Change Log

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 1.0 | Nov 6, 2025 | Initial specification | GitHub Copilot |

---

**End of Project Specification**

This document will be updated as requirements evolve and new features are planned. Please review and provide feedback before implementation begins.
