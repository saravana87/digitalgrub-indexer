
"""
Unified AI News Scraper
Sources:
  1. artificialintelligence-news.com
  2. techcrunch.com/category/artificial-intelligence/

Each source:
  - Extracts article links (with pagination)
  - Extracts content (<p>, <h3> etc.)
  - Saves to PostgreSQL (table: ai_news_articles)
"""

import os
import re
import hashlib
from datetime import datetime
from urllib.parse import urljoin, urlparse
import requests
from bs4 import BeautifulSoup
import psycopg2
from psycopg2 import sql
from dotenv import load_dotenv

load_dotenv()

HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36"
}

# ====================================================
# Utility functions
# ====================================================
def generate_content_hash(content: str) -> str:
    return hashlib.sha256((content or "").encode("utf-8")).hexdigest()

def get_db_connection():
    try:
        return psycopg2.connect(
            host=os.getenv("DB_HOST", "localhost"),
            database=os.getenv("DB_NAME", "news_scraper_db"),
            user=os.getenv("DB_USER"),
            password=os.getenv("DB_PASSWORD"),
            port=os.getenv("DB_PORT", "5432")
        )
    except Exception as e:
        print(f"DB connection failed: {e}")
        return None

def ensure_table(conn):
    ddl = """
    CREATE TABLE IF NOT EXISTS ai_news_articles (
        id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
        title VARCHAR(500) NOT NULL,
        url VARCHAR(1000) UNIQUE NOT NULL,
        content TEXT,
        content_hash VARCHAR(64) UNIQUE NOT NULL,
        category VARCHAR(100),
        source VARCHAR(100),
        scraped_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        index_status INTEGER DEFAULT 0,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );

    CREATE INDEX IF NOT EXISTS idx_ai_news_url ON ai_news_articles(url);
    CREATE INDEX IF NOT EXISTS idx_ai_content_hash ON ai_news_articles(content_hash);
    CREATE INDEX IF NOT EXISTS idx_ai_index_status ON ai_news_articles(index_status);
    CREATE INDEX IF NOT EXISTS idx_ai_scraped_date ON ai_news_articles(scraped_date);
    CREATE INDEX IF NOT EXISTS idx_ai_source ON ai_news_articles(source);
    """
    with conn.cursor() as cur:
        cur.execute(ddl)
    conn.commit()

def upsert_article(conn, title, url, content, category, source):
    content_hash = generate_content_hash(content)
    query = sql.SQL("""
        INSERT INTO ai_news_articles (title, url, content, content_hash, category, source, scraped_date, index_status)
        VALUES (%s, %s, %s, %s, %s, %s, %s, 0)
        ON CONFLICT (url) DO UPDATE SET
            title = EXCLUDED.title,
            content = EXCLUDED.content,
            category = EXCLUDED.category,
            source = EXCLUDED.source,
            updated_at = CURRENT_TIMESTAMP;
    """)
    with conn.cursor() as cur:
        cur.execute(query, (title, url, content, content_hash, category, source, datetime.now()))
    conn.commit()


# ====================================================
# Source 1: artificialintelligence-news.com
# ====================================================
def get_ai_news_links(listing_url, timeout=20, max_pages=50):
    all_links = []
    visited = set()
    failed_attempts = 0
    page_counter = 1

    while listing_url and listing_url not in visited and page_counter <= max_pages:
        visited.add(listing_url)
        print(f"[AI News] Page {page_counter}: {listing_url}")
        try:
            resp = requests.get(listing_url, headers=HEADERS, timeout=timeout)
            resp.raise_for_status()
        except Exception as e:
            print(f"Failed to fetch page: {e}")
            failed_attempts += 1
            if failed_attempts >= 3:
                break
            page_counter += 1
            continue

        soup = BeautifulSoup(resp.text, "lxml")
        page_links = [
            urljoin(listing_url, a["href"].strip())
            for a in soup.select("h1.elementor-heading-title a[href]")
        ]
        for l in page_links:
            if l not in all_links:
                all_links.append(l)

        next_tag = soup.select_one('nav.elementor-pagination a.page-numbers.next')
        if next_tag and next_tag.get("href"):
            listing_url = urljoin(listing_url, next_tag["href"].strip())
        else:
            match = re.search(r"([?&]e-page-[a-z0-9]+)=(\d+)", listing_url)
            if match:
                prefix, num = match.groups()
                next_num = int(num) + 1
                listing_url = re.sub(r"=\d+", f"={next_num}", listing_url)
            else:
                listing_url = listing_url.rstrip("/") + "?e-page-144a33e6=2"
        page_counter += 1

    print(f"Total AI News links: {len(all_links)}")
    return all_links

def scrape_ai_news_article(url):
    resp = requests.get(url, headers=HEADERS, timeout=25)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "lxml")

    title = soup.find("h1")
    title = title.get_text(strip=True) if title else url.split("/")[-2]
    container = soup.select_one("div.elementor-widget-theme-post-content")
    blocks = []
    if container:
        for node in container.find_all(["h3", "p"]):
            text = node.get_text(" ", strip=True)
            if node.name.lower() == "h3":
                blocks.append(f"### {text}")
            elif text:
                blocks.append(text)
    return title, "\n\n".join(blocks).strip()

# ====================================================
# Source 2: techcrunch.com/category/artificial-intelligence
# ====================================================
def get_techcrunch_links(base_url="https://techcrunch.com/category/artificial-intelligence/", timeout=20, max_pages=50):
    all_links = []
    visited = set()
    page_url = base_url
    page_count = 1

    while page_url and page_url not in visited and page_count <= max_pages:
        visited.add(page_url)
        print(f"[TechCrunch] Page {page_count}: {page_url}")
        try:
            resp = requests.get(page_url, headers=HEADERS, timeout=timeout)
            resp.raise_for_status()
        except Exception as e:
            print(f"Failed to fetch: {e}")
            break

        soup = BeautifulSoup(resp.text, "lxml")
        links = [
            a["href"].strip()
            for a in soup.select("h3.loop-card__title a.loop-card__title-link[href]")
            if a["href"].startswith("https://techcrunch.com/")
        ]
        for l in links:
            if l not in all_links:
                all_links.append(l)

        next_tag = soup.select_one('nav.wp-block-query-pagination a.wp-block-query-pagination-next')
        if next_tag and next_tag.get("href"):
            page_url = next_tag["href"]
            print(f"   âž¡ï¸ Found next page: {page_url}")
        else:
            # Fallback manual pagination guess
            page_count += 1
            page_url = base_url.rstrip("/") + f"/page/{page_count}/"
            print(f"   ðŸ” Guessing next page: {page_url}")

        page_count += 1

    print(f"âœ… Total TechCrunch links: {len(all_links)}")
    return all_links

def scrape_techcrunch_article(url):
    resp = requests.get(url, headers=HEADERS, timeout=25)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "lxml")

    title_tag = soup.find("h1")
    title = title_tag.get_text(strip=True) if title_tag else "Untitled"

    content_div = soup.select_one("div.wp-block-post-content")
    paragraphs = []
    if content_div:
        for p in content_div.find_all("p"):
            text = p.get_text(" ", strip=True)
            if text:
                paragraphs.append(text)

    return title, "\n\n".join(paragraphs).strip()

# ====================================================
# Source 3: indianexpress.com - Technology / Artificial Intelligence
# ====================================================
def get_indianexpress_links(listing_url, timeout=20, max_pages=50):
    """Return list of article links from Indian Express AI section, following simple pagination.

    Picks anchors inside `div.area-row a[href]` and uses the pagination block
    `div.ie-pagination ul.page-numbers` to find the next page. Returns absolute URLs.
    """
    all_links = []
    visited = set()
    page_url = listing_url
    page_count = 1

    while page_url and page_url not in visited and page_count <= max_pages:
        visited.add(page_url)
        print(f"[IndianExpress] Page {page_count}: {page_url}")
        try:
            resp = requests.get(page_url, headers=HEADERS, timeout=timeout)
            resp.raise_for_status()
        except Exception as e:
            print(f"Failed to fetch IndianExpress page: {e}")
            break

        soup = BeautifulSoup(resp.text, "lxml")

        # Find article anchors inside area-row blocks
        for a in soup.select("div.area-row a[href]"):
            try:
                href = a.get('href')
                if not href:
                    continue
                href = urljoin(page_url, href.strip())

                # Title is inside the anchor -> div.content-area h2.list-heading
                title_tag = a.select_one("h2.list-heading") or a.select_one("h2.list-heading ")
                title = title_tag.get_text(strip=True) if title_tag else None

                if href not in all_links:
                    all_links.append(href)
            except Exception:
                continue

        # Pagination: look for a numeric next page link in pagination block
        current_page = 1
        current_tag = soup.select_one("div.ie-pagination li span.current") or soup.select_one("div.ie-pagination li span.page-numbers.current")
        try:
            if current_tag:
                current_page = int(current_tag.get_text(strip=True))
        except Exception:
            current_page = page_count

        next_tag = None
        for a in soup.select("div.ie-pagination ul.page-numbers a.page-numbers[href]"):
            txt = a.get_text(strip=True)
            if txt.isdigit():
                try:
                    if int(txt) == current_page + 1:
                        next_tag = a
                        break
                except Exception:
                    continue

        if next_tag and next_tag.get("href"):
            page_url = urljoin(page_url, next_tag["href"].strip())
        else:
            # fallback: guess next page URL
            page_count += 1
            page_url = listing_url.rstrip("/") + f"/page/{page_count}/"

        page_count += 1

    print(f"Total IndianExpress links: {len(all_links)}")
    return all_links


def scrape_indianexpress_article(url):
    """Scrape Indian Express article title and content.

    - Title: first <h1>
    - Content: paragraphs inside `div#pcl-full-content` and any `div.ie-premium-content-block` within it.
    Returns (title, content_text)
    """
    resp = requests.get(url, headers=HEADERS, timeout=25)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "lxml")

    title_tag = soup.find("h1")
    title = title_tag.get_text(strip=True) if title_tag else "Untitled"

    content_container = soup.select_one("div#pcl-full-content")
    paragraphs = []

    if content_container:
        # main <p> tags
        for p in content_container.find_all("p"):
            text = p.get_text(" ", strip=True)
            if text and len(text) > 10:
                paragraphs.append(text)

        # include premium content blocks inside the same container
        for block in content_container.select("div.ie-premium-content-block"):
            for p in block.find_all("p"):
                text = p.get_text(" ", strip=True)
                if text and len(text) > 10:
                    paragraphs.append(text)

    else:
        # fallback: try generic article paragraphs
        for p in soup.select("article p"):
            text = p.get_text(" ", strip=True)
            if text and len(text) > 10:
                paragraphs.append(text)

    return title, "\n\n".join(paragraphs).strip()


# ====================================================
# Source 4: news.mit.edu - Artificial Intelligence topic
# ====================================================
def get_mit_links(listing_url, timeout=20, max_pages=50):
    """Return list of article links from MIT News AI topic.

    - Finds anchors with class `term-page--news-article--item--title--link`.
    - Uses `nav.views--pager a[rel=next]` as primary pagination, falls back to guessing `?page=n`.
    Returns absolute URLs (base: https://news.mit.edu).
    """
    base = "https://news.mit.edu"
    all_links = []
    visited = set()
    page_url = listing_url
    page_count = 0

    while page_url and page_url not in visited and page_count < max_pages:
        visited.add(page_url)
        page_count += 1
        print(f"[MIT News] Page {page_count}: {page_url}")
        try:
            resp = requests.get(page_url, headers=HEADERS, timeout=timeout)
            resp.raise_for_status()
        except Exception as e:
            print(f"Failed to fetch MIT page: {e}")
            break

        soup = BeautifulSoup(resp.text, "lxml")

        for a in soup.select("a.term-page--news-article--item--title--link[href]"):
            href = a.get('href')
            if not href:
                continue
            full = urljoin(base, href.strip())
            if full not in all_links:
                all_links.append(full)

        # Pagination: prefer rel=next anchor
        next_tag = soup.select_one('nav.views--pager a[rel="next"][href]') or soup.select_one('ul.js-pager__items a[rel="next"][href]')
        if next_tag and next_tag.get('href'):
            page_url = urljoin(base, next_tag.get('href').strip())
            continue

        # Fallback: try to guess next page by incrementing ?page=N
        parsed = page_url.split('?page=')
        if len(parsed) == 2:
            try:
                cur = int(parsed[1].split('&')[0])
                page_url = parsed[0] + f'?page={cur+1}'
            except Exception:
                break
        else:
            page_url = listing_url.rstrip('/') + '?page=1'

    print(f"Total MIT links: {len(all_links)}")
    return all_links


def scrape_mit_article(url):
    """Scrape MIT News article content.

    - Title: first <h1>
    - Content: collect all <p> tags inside `div.paragraph--type--content-block-text` containers.
    Returns (title, content_text)
    """
    resp = requests.get(url, headers=HEADERS, timeout=25)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "lxml")

    title_tag = soup.find('h1')
    title = title_tag.get_text(strip=True) if title_tag else 'Untitled'

    paragraphs = []
    for block in soup.select('div.paragraph--type--content-block-text'):
        for p in block.find_all('p'):
            text = p.get_text(' ', strip=True)
            if text:
                paragraphs.append(text)

    # Fallback: any article p tags
    if not paragraphs:
        for p in soup.select('article p'):
            text = p.get_text(' ', strip=True)
            if text:
                paragraphs.append(text)

    return title, "\n\n".join(paragraphs).strip()

# ====================================================
# MAIN EXECUTION
# ====================================================
def main():
    conn = get_db_connection()
    if not conn:
        print("Cannot connect to DB.")
        return
    ensure_table(conn)

    # ---- AI News ----
    # ai_links = get_ai_news_links("https://www.artificialintelligence-news.com/artificial-intelligence-news/")
    # for i, url in enumerate(ai_links, 1):
    #     print(f"[AI News {i}/{len(ai_links)}] Scraping: {url}")
    #     try:
    #         title, content = scrape_ai_news_article(url)
    #         if content:
    #             upsert_article(conn, title, url, content, "technology", "artificialintelligence-news.com")
    #     except Exception as e:
    #         print(f"Skipped due to error: {e}")

    # # ---- TechCrunch ----
    # tc_links = get_techcrunch_links()
    # for i, url in enumerate(tc_links, 1):
    #     print(f"[TechCrunch {i}/{len(tc_links)}] Scraping: {url}")
    #     try:
    #         title, content = scrape_techcrunch_article(url)
    #         if content:
    #             upsert_article(conn, title, url, content, "technology", "techcrunch.com")
    #     except Exception as e:
    #         print(f"Skipped due to error: {e}")

    # # ---- IndianExpress ----
    # ie_links = get_indianexpress_links(
    #     "https://indianexpress.com/section/technology/artificial-intelligence/"
    # )
    # for i, url in enumerate(ie_links, 1):
    #     print(f"[IndianExpress {i}/{len(ie_links)}] Scraping: {url}")
    #     try:
    #         title, content = scrape_indianexpress_article(url)
    #         if content:
    #             upsert_article(conn, title, url, content, "technology", "indianexpress.com")
    #     except Exception as e:
    #         print(f"Skipped due to error: {e}")

    # ---- MIT News ----
    mit_links = get_mit_links(
        "https://news.mit.edu/topic/artificial-intelligence2"
    )
    for i, url in enumerate(mit_links, 1):
        print(f"[MIT {i}/{len(mit_links)}] Scraping: {url}")
        try:
            title, content = scrape_mit_article(url)
            if content:
                upsert_article(conn, title, url, content, "technology", "news.mit.edu")
        except Exception as e:
            print(f"Skipped due to error: {e}")
    conn.close()
    print("All done.")

if __name__ == "__main__":
    main()